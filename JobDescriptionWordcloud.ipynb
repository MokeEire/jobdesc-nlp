{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Wordcloud in Python\n",
    "\n",
    "![](wc.png)\n",
    "\n",
    "Wordclouds are an interesting form of data visualization which show the prevalence of words in a given piece of text. \n",
    "In the context of applying for jobs, they seem particularly adept at identifying potential keywords (and/or *buzzwords*) which employers may be looking for in an application. \n",
    "It seems that the use of Applicant Tracking Systems (ATS) is becoming a major hurdle for jobseekers, with [tons](https://www.topresume.com/career-advice/what-is-an-ats-resume) [of](https://www.forbes.com/sites/nextavenue/2014/03/18/how-to-get-your-resume-read-by-an-employer/#346551266865) [articles](https://business.fullerton.edu/news/2018/05/29/applicant-tracking-systems-what-job-seekers-should-know/) outlining how to \"*optimize*\" one's resume for ATS to avoid being filtered out by algorithms.\n",
    "\n",
    "In the spirit of desiring fulfilling employment, and recognizing the importance of displaying the traits which an employer deems most important, I wanted to see which key words were showing up most often in the kinds of jobs I was applying to. \n",
    "It also seemed like a perfect opportunity to get my feet wet with text analysis in Python.\n",
    "\n",
    "I list the resources I used in the cell below, and try to note any place where code was shamelessly stolen. \n",
    "Anyway, let's get started and load in the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources:\n",
    "#   -   Text Analysis in Python: https://medium.com/towards-artificial-intelligence/text-mining-in-python-steps-and-examples-78b3f8fd913b\n",
    "#   -   Text Analysis overview: https://monkeylearn.com/text-analysis/\n",
    "#   -   https://towardsdatascience.com/nlp-for-beginners-cleaning-preprocessing-text-data-ae8e306bef0f\n",
    "#   -   https://towardsdatascience.com/basic-binary-sentiment-analysis-using-nltk-c94ba17ae386\n",
    "#   -   https://stackoverflow.com/questions/32957895/wordnetlemmatizer-not-returning-the-right-lemma-unless-pos-is-explicit-python\n",
    "\n",
    "\n",
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import re\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "import matplotlib as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions used to process the text data are:\n",
    "\n",
    "- **clean()** which removes characters which do not aid analysis - newline, dashes, slashes, etc.  \n",
    "- **remove_stopwords()** does what it says on the tin  \n",
    "- **word_lemmatizer()** reduces words to their root form e.g. looking => look  \n",
    "- **get_wordnet_pos()** is a function which tags words with their corresponding *part-of-speech*. This is necessary for lemmatization.  \n",
    "- **top_words()** finds the most commonly used words\n",
    "- **make_image()** creates the wordcloud and outputs a .png file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions -----------------------------------------------------------------------------------------------------------\n",
    "# Text cleaning function, shamelessly stolen from: https://github.com/datanizing/reddit-selfposts-blog\n",
    "def clean(s):\n",
    "    s = re.sub(r'((\\n)|(\\r))', \" \", s) # replace newline characters and \\r whatever it is (another line break char?) with spaces\n",
    "    s = re.sub(r'\\r(?=[A-Z].)', \"\", s) # remove \\r when it is next to a word\n",
    "    s = re.sub(r'/', \" \", s) # replace forward slashes with spaces\n",
    "    s = re.sub(r'\\-', \" \", s) # replace dashes with spaces (I will be forever cursed for not accounting for the em dash)\n",
    "    no_punct = \"\".join([c.lower() for c in s if c not in string.punctuation]) # remove punctuation\n",
    "\n",
    "    return no_punct\n",
    "\n",
    "# Function to remove stopwords from a list of words\n",
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "# Function to lemmatize strings from a list of words\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "# Lemmatizing reduces words to their root form\n",
    "def word_lemmatizer(text):\n",
    "    lem_text = [lemmatizer.lemmatize(word = i, pos= get_wordnet_pos(i)) for i in text]\n",
    "    return(lem_text)\n",
    "\n",
    "# Function for finding the most commonly used words in job desc\n",
    "def top_words(cleaned_desc, n = 3):\n",
    "    # Count word freq\n",
    "    freq = pd.Series(cleaned_desc.split()).value_counts()\n",
    "\n",
    "    # Select top 3 words\n",
    "    top_n = freq[:n].index.to_list()\n",
    "    return(top_n)\n",
    "\n",
    "# Function for creating masked wordcloud\n",
    "# Found here: https://amueller.github.io/word_cloud/auto_examples/masked.html\n",
    "def make_image(text, img):\n",
    "    # Need to get a mask image\n",
    "    mask = np.array(Image.open(img))\n",
    "\n",
    "    wc = WordCloud(background_color=\"white\", max_words=1000, mask=mask)\n",
    "    # generate word cloud\n",
    "    wc.generate_from_frequencies(text)\n",
    "\n",
    "    # show\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data\n",
    "\n",
    "First thing's first, we'll read in the data. \n",
    "This is a simple dataset of job postings I collected using Google Sheets. \n",
    "I collected the **Position**, **Company**, **Location**, **Role Description**, **Qualifications**, and **Benefits** (although benefits information was far less common)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Position', 'Company', 'Type', 'Location', 'Link', 'description', 'qualifications', 'benefits'], dtype='object')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data\n",
    "jobapps_file = 'data/jobapps.csv'\n",
    "jobapps_df = pd.read_csv(jobapps_file)\n",
    "\n",
    "jobapps_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Company</th>\n",
       "      <th>...</th>\n",
       "      <th>qualifications</th>\n",
       "      <th>benefits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Associate Governmental Program Analyst</td>\n",
       "      <td>Fair Employment Agency</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data and Policy Analyst - Statistical Programmer</td>\n",
       "      <td>Acumen LLC</td>\n",
       "      <td>...</td>\n",
       "      <td>Bachelor’s degree in a quantitative, public policy, or related field or equivalent relevant expe...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lead Business Intelligence Engineer</td>\n",
       "      <td>sweetgreen</td>\n",
       "      <td>...</td>\n",
       "      <td>Experience with modern data platforms (e.g. AWS, Snowflake, SQL, Airflow, Python)\\nExperience wi...</td>\n",
       "      <td>Three different medical plans to suit your and your family's needs\\nDental and Vision insurance\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Capacity Planning Analyst</td>\n",
       "      <td>Beyond Meat</td>\n",
       "      <td>...</td>\n",
       "      <td>5+ years of experience in operations or business analysis\\r\\nBachelor’s degree in statistics, ap...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data and Policy Analyst - Writer/Coordinator</td>\n",
       "      <td>Acumen LLC</td>\n",
       "      <td>...</td>\n",
       "      <td>Bachelor’s degree in a quantitative, public policy, or related field or equivalent relevant expe...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Position                 Company  ...                                                                                       qualifications                                                                                             benefits\n",
       "0            Associate Governmental Program Analyst  Fair Employment Agency  ...                                                                                                  NaN                                                                                                  NaN\n",
       "1  Data and Policy Analyst - Statistical Programmer              Acumen LLC  ...  Bachelor’s degree in a quantitative, public policy, or related field or equivalent relevant expe...                                                                                                  NaN\n",
       "2               Lead Business Intelligence Engineer              sweetgreen  ...  Experience with modern data platforms (e.g. AWS, Snowflake, SQL, Airflow, Python)\\nExperience wi...  Three different medical plans to suit your and your family's needs\\nDental and Vision insurance\\...\n",
       "3                         Capacity Planning Analyst             Beyond Meat  ...  5+ years of experience in operations or business analysis\\r\\nBachelor’s degree in statistics, ap...                                                                                                  NaN\n",
       "4      Data and Policy Analyst - Writer/Coordinator              Acumen LLC  ...  Bachelor’s degree in a quantitative, public policy, or related field or equivalent relevant expe...                                                                                                  NaN\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobapps_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate job description text\n",
    "job_desc = jobapps_df[['description']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "First we clean the text data. You can see the results below, this function removes punctuation, converts the text to lowercase and generally makes the text more machine-readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>desc_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30% Ensure that the DFEH complies with all OSHA/Cal-OSHA Regulations, in part by maintaining fam...</td>\n",
       "      <td>30 ensure that the dfeh complies with all osha cal osha regulations in part by maintaining famil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data and Policy Analysts perform a wide array of functions as part of the research process. Thos...</td>\n",
       "      <td>data and policy analysts perform a wide array of functions as part of the research process those...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lead BI Engineers are responsible for owning approximately 25% of our reporting portfolio - owni...</td>\n",
       "      <td>lead bi engineers are responsible for owning approximately 25 of our reporting portfolio   ownin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We are looking for an exceptional analyst who can diagnose and solve complex business problems t...</td>\n",
       "      <td>we are looking for an exceptional analyst who can diagnose and solve complex business problems t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data and Policy Analysts perform a wide array of functions as part of the research process. Thos...</td>\n",
       "      <td>data and policy analysts perform a wide array of functions as part of the research process those...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           description                                                                                           desc_clean\n",
       "0  30% Ensure that the DFEH complies with all OSHA/Cal-OSHA Regulations, in part by maintaining fam...  30 ensure that the dfeh complies with all osha cal osha regulations in part by maintaining famil...\n",
       "1  Data and Policy Analysts perform a wide array of functions as part of the research process. Thos...  data and policy analysts perform a wide array of functions as part of the research process those...\n",
       "2  Lead BI Engineers are responsible for owning approximately 25% of our reporting portfolio - owni...  lead bi engineers are responsible for owning approximately 25 of our reporting portfolio   ownin...\n",
       "3  We are looking for an exceptional analyst who can diagnose and solve complex business problems t...  we are looking for an exceptional analyst who can diagnose and solve complex business problems t...\n",
       "4  Data and Policy Analysts perform a wide array of functions as part of the research process. Thos...  data and policy analysts perform a wide array of functions as part of the research process those..."
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning\n",
    "job_desc_clean = job_desc\n",
    "job_desc_clean = job_desc_clean.assign(desc_clean = job_desc.description.apply(clean))\n",
    "\n",
    "job_desc_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "[Tokenization](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html) is the process of separating a sentence into smaller chunks such as words and number elements.\n",
    "\n",
    "Below I tokenize the job description text twice - once with stop words and once without, but this was mostly for my own edification to see the difference between the two. \n",
    "Stopwords are words which are fundamental to language but may not always be useful when analyzing text.  \n",
    "\n",
    "For example, you can see below by comparing `desc_clean` and `desc_clean_nostop` what kind of words are removed: \n",
    "\"*that*\", \"*the*\", \"*with*\", \"*a*\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>desc_clean</th>\n",
       "      <th>desc_tokenized</th>\n",
       "      <th>desc_clean_nostop</th>\n",
       "      <th>desc_tokenized_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30% Ensure that the DFEH complies with all OSHA/Cal-OSHA Regulations, in part by maintaining fam...</td>\n",
       "      <td>30 ensure that the dfeh complies with all osha cal osha regulations in part by maintaining famil...</td>\n",
       "      <td>[30, ensure, that, the, dfeh, complies, with, all, osha, cal, osha, regulations, in, part, by, m...</td>\n",
       "      <td>30 ensure dfeh complies osha cal osha regulations part maintaining familiarity current laws regu...</td>\n",
       "      <td>[30, ensure, dfeh, complies, osha, cal, osha, regulations, part, maintaining, familiarity, curre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data and Policy Analysts perform a wide array of functions as part of the research process. Thos...</td>\n",
       "      <td>data and policy analysts perform a wide array of functions as part of the research process those...</td>\n",
       "      <td>[data, and, policy, analysts, perform, a, wide, array, of, functions, as, part, of, the, researc...</td>\n",
       "      <td>data policy analysts perform wide array functions part research process applicants interested fo...</td>\n",
       "      <td>[data, policy, analysts, perform, wide, array, functions, part, research, process, applicants, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lead BI Engineers are responsible for owning approximately 25% of our reporting portfolio - owni...</td>\n",
       "      <td>lead bi engineers are responsible for owning approximately 25 of our reporting portfolio   ownin...</td>\n",
       "      <td>[lead, bi, engineers, are, responsible, for, owning, approximately, 25, of, our, reporting, port...</td>\n",
       "      <td>lead bi engineers responsible owning approximately 25 reporting portfolio owning exec level stak...</td>\n",
       "      <td>[lead, bi, engineers, responsible, owning, approximately, 25, reporting, portfolio, owning, exec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We are looking for an exceptional analyst who can diagnose and solve complex business problems t...</td>\n",
       "      <td>we are looking for an exceptional analyst who can diagnose and solve complex business problems t...</td>\n",
       "      <td>[we, are, looking, for, an, exceptional, analyst, who, can, diagnose, and, solve, complex, busin...</td>\n",
       "      <td>looking exceptional analyst diagnose solve complex business problems data analysis scenario mode...</td>\n",
       "      <td>[looking, exceptional, analyst, diagnose, solve, complex, business, problems, data, analysis, sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data and Policy Analysts perform a wide array of functions as part of the research process. Thos...</td>\n",
       "      <td>data and policy analysts perform a wide array of functions as part of the research process those...</td>\n",
       "      <td>[data, and, policy, analysts, perform, a, wide, array, of, functions, as, part, of, the, researc...</td>\n",
       "      <td>data policy analysts perform wide array functions part research process applicants interested fo...</td>\n",
       "      <td>[data, policy, analysts, perform, wide, array, functions, part, research, process, applicants, i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           description                                                                                           desc_clean  \\\n",
       "0  30% Ensure that the DFEH complies with all OSHA/Cal-OSHA Regulations, in part by maintaining fam...  30 ensure that the dfeh complies with all osha cal osha regulations in part by maintaining famil...   \n",
       "1  Data and Policy Analysts perform a wide array of functions as part of the research process. Thos...  data and policy analysts perform a wide array of functions as part of the research process those...   \n",
       "2  Lead BI Engineers are responsible for owning approximately 25% of our reporting portfolio - owni...  lead bi engineers are responsible for owning approximately 25 of our reporting portfolio   ownin...   \n",
       "3  We are looking for an exceptional analyst who can diagnose and solve complex business problems t...  we are looking for an exceptional analyst who can diagnose and solve complex business problems t...   \n",
       "4  Data and Policy Analysts perform a wide array of functions as part of the research process. Thos...  data and policy analysts perform a wide array of functions as part of the research process those...   \n",
       "\n",
       "                                                                                        desc_tokenized                                                                                    desc_clean_nostop  \\\n",
       "0  [30, ensure, that, the, dfeh, complies, with, all, osha, cal, osha, regulations, in, part, by, m...  30 ensure dfeh complies osha cal osha regulations part maintaining familiarity current laws regu...   \n",
       "1  [data, and, policy, analysts, perform, a, wide, array, of, functions, as, part, of, the, researc...  data policy analysts perform wide array functions part research process applicants interested fo...   \n",
       "2  [lead, bi, engineers, are, responsible, for, owning, approximately, 25, of, our, reporting, port...  lead bi engineers responsible owning approximately 25 reporting portfolio owning exec level stak...   \n",
       "3  [we, are, looking, for, an, exceptional, analyst, who, can, diagnose, and, solve, complex, busin...  looking exceptional analyst diagnose solve complex business problems data analysis scenario mode...   \n",
       "4  [data, and, policy, analysts, perform, a, wide, array, of, functions, as, part, of, the, researc...  data policy analysts perform wide array functions part research process applicants interested fo...   \n",
       "\n",
       "                                                                                 desc_tokenized_nostop  \n",
       "0  [30, ensure, dfeh, complies, osha, cal, osha, regulations, part, maintaining, familiarity, curre...  \n",
       "1  [data, policy, analysts, perform, wide, array, functions, part, research, process, applicants, i...  \n",
       "2  [lead, bi, engineers, responsible, owning, approximately, 25, reporting, portfolio, owning, exec...  \n",
       "3  [looking, exceptional, analyst, diagnose, solve, complex, business, problems, data, analysis, sc...  \n",
       "4  [data, policy, analysts, perform, wide, array, functions, part, research, process, applicants, i...  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tokenizing\n",
    "# Instantiate Tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Add tokenized column\n",
    "job_desc_clean['desc_tokenized'] = job_desc_clean.desc_clean.apply(lambda x: tokenizer.tokenize(x))\n",
    "\n",
    "# Remove stop words\n",
    "job_desc_clean['desc_clean_nostop'] = job_desc_clean['desc_clean'].apply(lambda x: \" \".join(x for x in x.split() if x not in stopwords.words('english')))\n",
    "# Add tokenized column w/o stop words\n",
    "job_desc_clean['desc_tokenized_nostop'] = job_desc_clean.desc_tokenized.apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "job_desc_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the text is tokenized, we can create a frequency distribution to see how often a particular word comes up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FreqDist({'dfeh': 5, 'regulations': 5, 'plans': 5, 'services': 5, 'state': 5, 'contract': 5, 'purchase': 5, 'maintain': 5, 'evacuation': 4, 'coordinate': 4, ...}),\n",
       " FreqDist({'research': 3, 'statistical': 3, 'data': 2, 'perform': 2, 'analyses': 2, 'policy': 1, 'analysts': 1, 'wide': 1, 'array': 1, 'functions': 1, ...}),\n",
       " FreqDist({'data': 10, 'customer': 8, 'within': 5, 'reporting': 4, 'marketing': 4, 'teams': 3, 'customers': 3, 'days': 3, 'owning': 2, 'portfolio': 2, ...}),\n",
       " FreqDist({'capacity': 5, 'multiple': 4, 'global': 4, 'production': 4, 'business': 3, 'data': 3, 'planning': 3, 'worldwide': 3, 'ability': 3, 'analyst': 2, ...}),\n",
       " FreqDist({'research': 3, 'findings': 3, 'perform': 2, 'project': 2, 'clients': 2, 'data': 1, 'policy': 1, 'analysts': 1, 'wide': 1, 'array': 1, ...}),\n",
       " FreqDist({'data': 6, 'business': 5, 'insights': 5, 'analytics': 4, 'work': 3, 'player': 3, 'call': 2, 'duty': 2, 'mobile': 2, 'activision': 2, ...}),\n",
       " FreqDist({'business': 11, 'data': 7, 'sales': 6, 'financial': 5, 'erp': 5, 'analyze': 5, 'analyst': 4, 'analysis': 4, 'performance': 4, 'requirements': 4, ...}),\n",
       " FreqDist({'data': 11, 'security': 4, 'management': 4, 'portfolio': 4, 'trading': 3, 'analytics': 3, 'experience': 2, 'attributes': 2, 'including': 2, 'risk': 2, ...}),\n",
       " FreqDist({'business': 3, 'tools': 3, 'role': 2, 'reporting': 2, 'high': 2, 'stakeholders': 2, 'data': 2, 'driven': 2, 'key': 2, 'partners': 2, ...}),\n",
       " FreqDist({'operations': 5, 'strategy': 4, 'eaze': 4, 'business': 4, 'team': 3, 'cross': 3, 'functional': 3, 'analysis': 3, 'processes': 3, 'central': 2, ...}),\n",
       " FreqDist({'data': 16, 'reporting': 4, 'performance': 4, 'quality': 4, 'support': 4, 'analyst': 3, 'bail': 3, 'analysis': 3, 'organization': 3, 'tbp': 3, ...}),\n",
       " FreqDist({'data': 11, 'business': 7, 'across': 3, 'team': 3, 'intelligence': 2, 'analyst': 2, 'product': 2, 'part': 2, 'focused': 2, 'work': 2, ...}),\n",
       " FreqDist({'data': 3, 'manager': 2, 'project': 2, 'research': 2, 'perform': 2, 'related': 2, 'attention': 2, 'studies': 2, 'projects': 2, 'include': 2, ...}),\n",
       " FreqDist({'consumer': 3, 'documents': 3, 'conducting': 3, 'section': 3, 'assisting': 3, 'review': 2, 'analysis': 2, 'data': 2, 'practices': 2, 'complex': 2, ...}),\n",
       " FreqDist({'policy': 7, 'work': 5, 'data': 5, 'public': 4, 'teams': 3, 'support': 3, 'team': 3, 'seal': 2, 'ensure': 2, 'would': 2, ...}),\n",
       " FreqDist({'client': 3, 'work': 2, 'trss': 2, 'practices': 2, 'analysis': 2, 'data': 2, 'analytic': 2, 'produce': 1, 'regularly': 1, 'scheduled': 1, ...})]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the frequency distinct in the tokens\n",
    "# Importing FreqDist library from nltk and passing token into FreqDist\n",
    "from nltk.probability import FreqDist\n",
    "job_desc_freq = [FreqDist(desc) for desc in job_desc_clean.desc_tokenized_nostop]\n",
    "job_desc_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simplify and find the 10 most common words in each job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('dfeh', 5),\n",
       "  ('regulations', 5),\n",
       "  ('plans', 5),\n",
       "  ('services', 5),\n",
       "  ('state', 5),\n",
       "  ('contract', 5),\n",
       "  ('purchase', 5),\n",
       "  ('maintain', 5),\n",
       "  ('evacuation', 4),\n",
       "  ('coordinate', 4)],\n",
       " [('research', 3),\n",
       "  ('statistical', 3),\n",
       "  ('data', 2),\n",
       "  ('perform', 2),\n",
       "  ('analyses', 2),\n",
       "  ('policy', 1),\n",
       "  ('analysts', 1),\n",
       "  ('wide', 1),\n",
       "  ('array', 1),\n",
       "  ('functions', 1)],\n",
       " [('data', 10),\n",
       "  ('customer', 8),\n",
       "  ('within', 5),\n",
       "  ('reporting', 4),\n",
       "  ('marketing', 4),\n",
       "  ('teams', 3),\n",
       "  ('customers', 3),\n",
       "  ('days', 3),\n",
       "  ('owning', 2),\n",
       "  ('portfolio', 2)],\n",
       " [('capacity', 5),\n",
       "  ('multiple', 4),\n",
       "  ('global', 4),\n",
       "  ('production', 4),\n",
       "  ('business', 3),\n",
       "  ('data', 3),\n",
       "  ('planning', 3),\n",
       "  ('worldwide', 3),\n",
       "  ('ability', 3),\n",
       "  ('analyst', 2)],\n",
       " [('research', 3),\n",
       "  ('findings', 3),\n",
       "  ('perform', 2),\n",
       "  ('project', 2),\n",
       "  ('clients', 2),\n",
       "  ('data', 1),\n",
       "  ('policy', 1),\n",
       "  ('analysts', 1),\n",
       "  ('wide', 1),\n",
       "  ('array', 1)],\n",
       " [('data', 6),\n",
       "  ('business', 5),\n",
       "  ('insights', 5),\n",
       "  ('analytics', 4),\n",
       "  ('work', 3),\n",
       "  ('player', 3),\n",
       "  ('call', 2),\n",
       "  ('duty', 2),\n",
       "  ('mobile', 2),\n",
       "  ('activision', 2)],\n",
       " [('business', 11),\n",
       "  ('data', 7),\n",
       "  ('sales', 6),\n",
       "  ('financial', 5),\n",
       "  ('erp', 5),\n",
       "  ('analyze', 5),\n",
       "  ('analyst', 4),\n",
       "  ('analysis', 4),\n",
       "  ('performance', 4),\n",
       "  ('requirements', 4)],\n",
       " [('data', 11),\n",
       "  ('security', 4),\n",
       "  ('management', 4),\n",
       "  ('portfolio', 4),\n",
       "  ('trading', 3),\n",
       "  ('analytics', 3),\n",
       "  ('experience', 2),\n",
       "  ('attributes', 2),\n",
       "  ('including', 2),\n",
       "  ('risk', 2)],\n",
       " [('business', 3),\n",
       "  ('tools', 3),\n",
       "  ('role', 2),\n",
       "  ('reporting', 2),\n",
       "  ('high', 2),\n",
       "  ('stakeholders', 2),\n",
       "  ('data', 2),\n",
       "  ('driven', 2),\n",
       "  ('key', 2),\n",
       "  ('partners', 2)],\n",
       " [('operations', 5),\n",
       "  ('strategy', 4),\n",
       "  ('eaze', 4),\n",
       "  ('business', 4),\n",
       "  ('team', 3),\n",
       "  ('cross', 3),\n",
       "  ('functional', 3),\n",
       "  ('analysis', 3),\n",
       "  ('processes', 3),\n",
       "  ('central', 2)],\n",
       " [('data', 16),\n",
       "  ('reporting', 4),\n",
       "  ('performance', 4),\n",
       "  ('quality', 4),\n",
       "  ('support', 4),\n",
       "  ('analyst', 3),\n",
       "  ('bail', 3),\n",
       "  ('analysis', 3),\n",
       "  ('organization', 3),\n",
       "  ('tbp', 3)],\n",
       " [('data', 11),\n",
       "  ('business', 7),\n",
       "  ('across', 3),\n",
       "  ('team', 3),\n",
       "  ('intelligence', 2),\n",
       "  ('analyst', 2),\n",
       "  ('product', 2),\n",
       "  ('part', 2),\n",
       "  ('focused', 2),\n",
       "  ('work', 2)],\n",
       " [('data', 3),\n",
       "  ('manager', 2),\n",
       "  ('project', 2),\n",
       "  ('research', 2),\n",
       "  ('perform', 2),\n",
       "  ('related', 2),\n",
       "  ('attention', 2),\n",
       "  ('studies', 2),\n",
       "  ('projects', 2),\n",
       "  ('include', 2)],\n",
       " [('consumer', 3),\n",
       "  ('documents', 3),\n",
       "  ('conducting', 3),\n",
       "  ('section', 3),\n",
       "  ('assisting', 3),\n",
       "  ('review', 2),\n",
       "  ('analysis', 2),\n",
       "  ('data', 2),\n",
       "  ('practices', 2),\n",
       "  ('complex', 2)],\n",
       " [('policy', 7),\n",
       "  ('work', 5),\n",
       "  ('data', 5),\n",
       "  ('public', 4),\n",
       "  ('teams', 3),\n",
       "  ('support', 3),\n",
       "  ('team', 3),\n",
       "  ('seal', 2),\n",
       "  ('ensure', 2),\n",
       "  ('would', 2)],\n",
       " [('client', 3),\n",
       "  ('work', 2),\n",
       "  ('trss', 2),\n",
       "  ('practices', 2),\n",
       "  ('analysis', 2),\n",
       "  ('data', 2),\n",
       "  ('analytic', 2),\n",
       "  ('produce', 1),\n",
       "  ('regularly', 1),\n",
       "  ('scheduled', 1)]]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To find the frequency of top 10 words\n",
    "desc_most_common = [fdist.most_common(10) for fdist in job_desc_freq]\n",
    "desc_most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>desc_clean</th>\n",
       "      <th>...</th>\n",
       "      <th>desc_tokenized_nostop</th>\n",
       "      <th>desc_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30% Ensure that the DFEH complies with all OSHA/Cal-OSHA Regulations, in part by maintaining fam...</td>\n",
       "      <td>30 ensure that the dfeh complies with all osha cal osha regulations in part by maintaining famil...</td>\n",
       "      <td>...</td>\n",
       "      <td>[30, ensure, dfeh, complies, osha, cal, osha, regulations, part, maintaining, familiarity, curre...</td>\n",
       "      <td>[30, ensure, dfeh, complies, osha, cal, osha, regulation, part, maintain, familiarity, current, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data and Policy Analysts perform a wide array of functions as part of the research process. Thos...</td>\n",
       "      <td>data and policy analysts perform a wide array of functions as part of the research process those...</td>\n",
       "      <td>...</td>\n",
       "      <td>[data, policy, analysts, perform, wide, array, functions, part, research, process, applicants, i...</td>\n",
       "      <td>[data, policy, analyst, perform, wide, array, function, part, research, process, applicant, inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lead BI Engineers are responsible for owning approximately 25% of our reporting portfolio - owni...</td>\n",
       "      <td>lead bi engineers are responsible for owning approximately 25 of our reporting portfolio   ownin...</td>\n",
       "      <td>...</td>\n",
       "      <td>[lead, bi, engineers, responsible, owning, approximately, 25, reporting, portfolio, owning, exec...</td>\n",
       "      <td>[lead, bi, engineer, responsible, own, approximately, 25, reporting, portfolio, own, exec, level...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We are looking for an exceptional analyst who can diagnose and solve complex business problems t...</td>\n",
       "      <td>we are looking for an exceptional analyst who can diagnose and solve complex business problems t...</td>\n",
       "      <td>...</td>\n",
       "      <td>[looking, exceptional, analyst, diagnose, solve, complex, business, problems, data, analysis, sc...</td>\n",
       "      <td>[look, exceptional, analyst, diagnose, solve, complex, business, problem, data, analysis, scenar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data and Policy Analysts perform a wide array of functions as part of the research process. Thos...</td>\n",
       "      <td>data and policy analysts perform a wide array of functions as part of the research process those...</td>\n",
       "      <td>...</td>\n",
       "      <td>[data, policy, analysts, perform, wide, array, functions, part, research, process, applicants, i...</td>\n",
       "      <td>[data, policy, analyst, perform, wide, array, function, part, research, process, applicant, inte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           description                                                                                           desc_clean  ...  \\\n",
       "0  30% Ensure that the DFEH complies with all OSHA/Cal-OSHA Regulations, in part by maintaining fam...  30 ensure that the dfeh complies with all osha cal osha regulations in part by maintaining famil...  ...   \n",
       "1  Data and Policy Analysts perform a wide array of functions as part of the research process. Thos...  data and policy analysts perform a wide array of functions as part of the research process those...  ...   \n",
       "2  Lead BI Engineers are responsible for owning approximately 25% of our reporting portfolio - owni...  lead bi engineers are responsible for owning approximately 25 of our reporting portfolio   ownin...  ...   \n",
       "3  We are looking for an exceptional analyst who can diagnose and solve complex business problems t...  we are looking for an exceptional analyst who can diagnose and solve complex business problems t...  ...   \n",
       "4  Data and Policy Analysts perform a wide array of functions as part of the research process. Thos...  data and policy analysts perform a wide array of functions as part of the research process those...  ...   \n",
       "\n",
       "                                                                                 desc_tokenized_nostop                                                                                      desc_lemmatized  \n",
       "0  [30, ensure, dfeh, complies, osha, cal, osha, regulations, part, maintaining, familiarity, curre...  [30, ensure, dfeh, complies, osha, cal, osha, regulation, part, maintain, familiarity, current, ...  \n",
       "1  [data, policy, analysts, perform, wide, array, functions, part, research, process, applicants, i...  [data, policy, analyst, perform, wide, array, function, part, research, process, applicant, inte...  \n",
       "2  [lead, bi, engineers, responsible, owning, approximately, 25, reporting, portfolio, owning, exec...  [lead, bi, engineer, responsible, own, approximately, 25, reporting, portfolio, own, exec, level...  \n",
       "3  [looking, exceptional, analyst, diagnose, solve, complex, business, problems, data, analysis, sc...  [look, exceptional, analyst, diagnose, solve, complex, business, problem, data, analysis, scenar...  \n",
       "4  [data, policy, analysts, perform, wide, array, functions, part, research, process, applicants, i...  [data, policy, analyst, perform, wide, array, function, part, research, process, applicant, inte...  \n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lemmatization\n",
    "# Importing Lemmatizer library from nltk\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Add lemmatized column\n",
    "job_desc_clean['desc_lemmatized'] = job_desc_clean.desc_tokenized_nostop.apply(lambda x: word_lemmatizer(x))\n",
    "\n",
    "job_desc_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that the lemmatization isn't completely accurate e.g. complies is not lemmatized to *comply* and reporting does not become *report*. \n",
    "To be honest, I am not completely sure why this happens.  The *part-of-speech* tagging is not tagging as expected in a number of cases, and that is something I will have to investigate in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count word frequencies\n",
    "\n",
    "So, what words occur most frequently?\n",
    "\n",
    "Well we have a number of different text columns which we can look at. First let's simply look at the cleaned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "and         288\n",
       "to          118\n",
       "the         108\n",
       "of           89\n",
       "data         82\n",
       "in           51\n",
       "with         48\n",
       "for          45\n",
       "business     39\n",
       "our          37\n",
       "dtype: int64"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count word frequencies\n",
    "freq = pd.Series(' '.join(job_desc_clean['desc_clean']).split()).value_counts()[:10]\n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we get a lot of stop words. This is one reason why we remove them. \n",
    "So what about the cleaned text without the stop words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data              82\n",
       "business          39\n",
       "analysis          22\n",
       "work              21\n",
       "reports           17\n",
       "                  ..\n",
       "idea               1\n",
       "inspections        1\n",
       "approach           1\n",
       "transportation     1\n",
       "bar                1\n",
       "Length: 1111, dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count word freq w/o stop words\n",
    "freq_nostop = pd.Series(' '.join(job_desc_clean['desc_clean_nostop']).split()).value_counts()\n",
    "\n",
    "freq_nostop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're starting to get a little insight into the content of the job descriptions, **data**, **business**, **analysis**, and **work** are our most common words. As you might be able to tell, I am looking primarily at jobs which leverage data analysis.\n",
    "\n",
    "How does this differ when we use lemmatized words instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data          82\n",
       "business      39\n",
       "analysis      30\n",
       "team          27\n",
       "work          27\n",
       "              ..\n",
       "capability     1\n",
       "12             1\n",
       "good           1\n",
       "instal         1\n",
       "40lbs          1\n",
       "Length: 933, dtype: int64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_lemma = pd.Series(' '.join(job_desc_clean['desc_lemmatized'].apply(lambda x: ' '.join(x))).split()).value_counts()\n",
    "\n",
    "freq_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that there is a little inconsistency when switching between applying functions to `desc_clean_nostop` and `desc_lemmatized`. This is due to the two being different data types - a string, and a list respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Company</th>\n",
       "      <th>top_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Associate Governmental Program Analyst</td>\n",
       "      <td>Fair Employment Agency</td>\n",
       "      <td>[state, purchase, services, regulations, contract]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data and Policy Analyst - Statistical Programmer</td>\n",
       "      <td>Acumen LLC</td>\n",
       "      <td>[statistical, research, perform, data, analyses]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lead Business Intelligence Engineer</td>\n",
       "      <td>sweetgreen</td>\n",
       "      <td>[data, customer, within, marketing, reporting]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Capacity Planning Analyst</td>\n",
       "      <td>Beyond Meat</td>\n",
       "      <td>[capacity, production, multiple, global, business]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data and Policy Analyst - Writer/Coordinator</td>\n",
       "      <td>Acumen LLC</td>\n",
       "      <td>[research, findings, project, perform, clients]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Position                 Company                                           top_words\n",
       "0            Associate Governmental Program Analyst  Fair Employment Agency  [state, purchase, services, regulations, contract]\n",
       "1  Data and Policy Analyst - Statistical Programmer              Acumen LLC    [statistical, research, perform, data, analyses]\n",
       "2               Lead Business Intelligence Engineer              sweetgreen      [data, customer, within, marketing, reporting]\n",
       "3                         Capacity Planning Analyst             Beyond Meat  [capacity, production, multiple, global, business]\n",
       "4      Data and Policy Analyst - Writer/Coordinator              Acumen LLC     [research, findings, project, perform, clients]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select top words for each\n",
    "job_desc_clean['top_words'] = job_desc_clean.desc_clean_nostop.apply(lambda x: top_words(x, 5))\n",
    "\n",
    "# Join back to the job data to see each position's most common terms\n",
    "jobapps_df.iloc[:,0:2].join(job_desc_clean.top_words).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Company</th>\n",
       "      <th>top_words_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Associate Governmental Program Analyst</td>\n",
       "      <td>Fair Employment Agency</td>\n",
       "      <td>[contract, maintain, office, service, dfeh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data and Policy Analyst - Statistical Programmer</td>\n",
       "      <td>Acumen LLC</td>\n",
       "      <td>[statistical, research, perform, data, analysis]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lead Business Intelligence Engineer</td>\n",
       "      <td>sweetgreen</td>\n",
       "      <td>[customer, data, within, reporting, marketing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Capacity Planning Analyst</td>\n",
       "      <td>Beyond Meat</td>\n",
       "      <td>[capacity, multiple, global, production, business]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data and Policy Analyst - Writer/Coordinator</td>\n",
       "      <td>Acumen LLC</td>\n",
       "      <td>[finding, research, report, perform, coordinate]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Position                 Company                                     top_words_lemma\n",
       "0            Associate Governmental Program Analyst  Fair Employment Agency         [contract, maintain, office, service, dfeh]\n",
       "1  Data and Policy Analyst - Statistical Programmer              Acumen LLC    [statistical, research, perform, data, analysis]\n",
       "2               Lead Business Intelligence Engineer              sweetgreen      [customer, data, within, reporting, marketing]\n",
       "3                         Capacity Planning Analyst             Beyond Meat  [capacity, multiple, global, production, business]\n",
       "4      Data and Policy Analyst - Writer/Coordinator              Acumen LLC    [finding, research, report, perform, coordinate]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What about the top words using lemmatized descriptions?\n",
    "job_desc_clean['top_words_lemma'] = job_desc_clean.desc_lemmatized.apply(lambda x: ' '.join(x)).apply(lambda x: top_words(x, 5))\n",
    "\n",
    "jobapps_df.iloc[:,0:2].join(job_desc_clean.top_words_lemma).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we have our top words, we can make the wordcloud. To do this, we will use the [wordcloud](https://pypi.org/project/wordcloud/) library. You can find a little tutorial on how to use it [here](https://www.geeksforgeeks.org/generating-word-cloud-python/). \n",
    "\n",
    "First we convert our word frequencies into a dictionary, and then we pass it to the `make_image()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': 82,\n",
       " 'business': 39,\n",
       " 'analysis': 30,\n",
       " 'team': 27,\n",
       " 'work': 27,\n",
       " 'report': 20,\n",
       " 'analyst': 19,\n",
       " 'support': 18,\n",
       " 'develop': 18,\n",
       " 'reporting': 16,\n",
       " 'insight': 16,\n",
       " 'provide': 16,\n",
       " 'include': 15,\n",
       " 'research': 14,\n",
       " 'project': 13,\n",
       " 'customer': 13,\n",
       " 'service': 12,\n",
       " 'process': 12,\n",
       " 'policy': 12,\n",
       " 'analytics': 12,\n",
       " 'product': 12,\n",
       " 'use': 12,\n",
       " 'across': 12,\n",
       " 'contract': 12,\n",
       " 'system': 11,\n",
       " 'related': 11,\n",
       " 'analyze': 11,\n",
       " 'program': 11,\n",
       " 'performance': 10,\n",
       " 'drive': 10,\n",
       " 'plan': 10,\n",
       " 'ensure': 10,\n",
       " 'manager': 10,\n",
       " 'operation': 10,\n",
       " 'management': 10,\n",
       " 'within': 10,\n",
       " 'role': 9,\n",
       " 'organization': 9,\n",
       " 'prepare': 9,\n",
       " 'build': 9,\n",
       " 'need': 9,\n",
       " 'client': 9,\n",
       " 'stakeholder': 9,\n",
       " 'sale': 8,\n",
       " 'perform': 8,\n",
       " 'requirement': 8,\n",
       " 'office': 8,\n",
       " 'complex': 8,\n",
       " 'public': 8,\n",
       " 'maintain': 8,\n",
       " 'experience': 8,\n",
       " 'development': 7,\n",
       " 'model': 7,\n",
       " 'new': 7,\n",
       " 'planning': 7,\n",
       " 'manage': 7,\n",
       " 'internal': 7,\n",
       " 'coordinate': 7,\n",
       " 'financial': 7,\n",
       " 'conduct': 7,\n",
       " 'source': 7,\n",
       " 'multiple': 7,\n",
       " 'tool': 7,\n",
       " 'cross': 7,\n",
       " 'operational': 7,\n",
       " 'require': 7,\n",
       " 'problem': 7,\n",
       " 'focus': 6,\n",
       " 'duty': 6,\n",
       " 'portfolio': 6,\n",
       " 'high': 6,\n",
       " 'issue': 6,\n",
       " 'measure': 6,\n",
       " 'ability': 6,\n",
       " 'record': 6,\n",
       " 'part': 6,\n",
       " 'dashboard': 6,\n",
       " 'closely': 6,\n",
       " 'goal': 6,\n",
       " 'finance': 6,\n",
       " 'request': 6,\n",
       " 'generate': 6,\n",
       " 'key': 6,\n",
       " 'driven': 6,\n",
       " 'strategy': 6,\n",
       " 'state': 6,\n",
       " 'quality': 6,\n",
       " 'inform': 6,\n",
       " 'finding': 5,\n",
       " 'opportunity': 5,\n",
       " 'day': 5,\n",
       " 'sql': 5,\n",
       " 'capacity': 5,\n",
       " 'change': 5,\n",
       " 'software': 5,\n",
       " 'site': 5,\n",
       " 'recommendation': 5,\n",
       " 'engagement': 5,\n",
       " 'practice': 5,\n",
       " 'quantitative': 5,\n",
       " 'assist': 5,\n",
       " 'implement': 5,\n",
       " 'understand': 5,\n",
       " 'knowledge': 5,\n",
       " 'create': 5,\n",
       " 'improve': 5,\n",
       " 'lead': 5,\n",
       " 'relevant': 5,\n",
       " 'procedure': 5,\n",
       " 'partner': 5,\n",
       " 'metric': 5,\n",
       " 'security': 5,\n",
       " 'dfeh': 5,\n",
       " 'erp': 5,\n",
       " 'regulation': 5,\n",
       " 'document': 5,\n",
       " 'functional': 5,\n",
       " 'global': 5,\n",
       " 'success': 5,\n",
       " 'purchase': 5,\n",
       " 'department': 5,\n",
       " 'help': 5,\n",
       " 'analytical': 4,\n",
       " 'technical': 4,\n",
       " 'supply': 4,\n",
       " 'procurement': 4,\n",
       " 'write': 4,\n",
       " 'technique': 4,\n",
       " 'u': 4,\n",
       " 'order': 4,\n",
       " 'statistic': 4,\n",
       " 'statistical': 4,\n",
       " 'look': 4,\n",
       " 'assign': 4,\n",
       " 'attention': 4,\n",
       " 'expert': 4,\n",
       " 'design': 4,\n",
       " 'wide': 4,\n",
       " 'function': 4,\n",
       " 'study': 4,\n",
       " 'descriptive': 4,\n",
       " 'response': 4,\n",
       " 'strategic': 4,\n",
       " 'activity': 4,\n",
       " 'integrate': 4,\n",
       " 'initiative': 4,\n",
       " 'responsible': 4,\n",
       " 'various': 4,\n",
       " 'keep': 4,\n",
       " 'line': 4,\n",
       " 'eaze': 4,\n",
       " 'communication': 4,\n",
       " 'detail': 4,\n",
       " 'hoc': 4,\n",
       " 'solution': 4,\n",
       " 'skill': 4,\n",
       " 'building': 4,\n",
       " 'also': 4,\n",
       " 'play': 4,\n",
       " 'marketing': 4,\n",
       " 'query': 4,\n",
       " 'evacuation': 4,\n",
       " 'base': 4,\n",
       " 'staff': 4,\n",
       " 'position': 4,\n",
       " 'production': 4,\n",
       " 'workflow': 4,\n",
       " 'leverage': 4,\n",
       " 'environment': 4,\n",
       " 'level': 4,\n",
       " 'engineering': 4,\n",
       " 'decision': 4,\n",
       " 'ad': 4,\n",
       " 'company': 4,\n",
       " 'flow': 4,\n",
       " 'call': 3,\n",
       " 'priority': 3,\n",
       " 'analytic': 3,\n",
       " 'candidate': 3,\n",
       " 'critical': 3,\n",
       " 'identify': 3,\n",
       " 'information': 3,\n",
       " 'external': 3,\n",
       " 'training': 3,\n",
       " 'impact': 3,\n",
       " 'successful': 3,\n",
       " 'travel': 3,\n",
       " 'area': 3,\n",
       " 'platform': 3,\n",
       " 'deputy': 3,\n",
       " 'exist': 3,\n",
       " 'respond': 3,\n",
       " 'intelligence': 3,\n",
       " 'initiate': 3,\n",
       " 'next': 3,\n",
       " 'effort': 3,\n",
       " 'grow': 3,\n",
       " 'may': 3,\n",
       " 'player': 3,\n",
       " 'innovation': 3,\n",
       " 'tbp': 3,\n",
       " 'sap': 3,\n",
       " 'audience': 3,\n",
       " 'delivery': 3,\n",
       " 'improvement': 3,\n",
       " 'trading': 3,\n",
       " 'worldwide': 3,\n",
       " 'party': 3,\n",
       " 'right': 3,\n",
       " 'responsibility': 3,\n",
       " 'serve': 3,\n",
       " 'regular': 3,\n",
       " 'special': 3,\n",
       " 'studio': 3,\n",
       " 'crm': 3,\n",
       " 'solve': 3,\n",
       " 'specification': 3,\n",
       " 'database': 3,\n",
       " 'best': 3,\n",
       " 'different': 3,\n",
       " 'trend': 3,\n",
       " 'way': 3,\n",
       " 'consumer': 3,\n",
       " 'code': 3,\n",
       " 'mission': 3,\n",
       " 'well': 3,\n",
       " 'enable': 3,\n",
       " 'log': 3,\n",
       " 'mail': 3,\n",
       " 'emergency': 3,\n",
       " 'fast': 3,\n",
       " 'actionable': 3,\n",
       " 'scenario': 3,\n",
       " 'kpis': 3,\n",
       " 'manual': 3,\n",
       " 'step': 3,\n",
       " 'central': 3,\n",
       " 'employee': 3,\n",
       " 'review': 3,\n",
       " 'equipment': 3,\n",
       " 'third': 3,\n",
       " 'self': 3,\n",
       " 'section': 3,\n",
       " 'bail': 3,\n",
       " 'join': 3,\n",
       " 'increase': 2,\n",
       " 'schedule': 2,\n",
       " 'activision': 2,\n",
       " 'set': 2,\n",
       " 'access': 2,\n",
       " 'rev': 2,\n",
       " 'seal': 2,\n",
       " 'unlock': 2,\n",
       " 'effective': 2,\n",
       " 'evaluate': 2,\n",
       " 'push': 2,\n",
       " 'pull': 2,\n",
       " 'accounting': 2,\n",
       " 'attainment': 2,\n",
       " 'execution': 2,\n",
       " 'advocate': 2,\n",
       " 'accurate': 2,\n",
       " 'technology': 2,\n",
       " 'sweetgreen': 2,\n",
       " 'summarize': 2,\n",
       " 'optimize': 2,\n",
       " 'idea': 2,\n",
       " 'integrity': 2,\n",
       " 'action': 2,\n",
       " 'would': 2,\n",
       " 'gathering': 2,\n",
       " 'partnership': 2,\n",
       " 'current': 2,\n",
       " 'culture': 2,\n",
       " 'limited': 2,\n",
       " 'broader': 2,\n",
       " 'chain': 2,\n",
       " 'entry': 2,\n",
       " 'mobile': 2,\n",
       " 'law': 2,\n",
       " 'among': 2,\n",
       " 'empower': 2,\n",
       " 'participate': 2,\n",
       " 'expect': 2,\n",
       " 'facilitate': 2,\n",
       " 'streamline': 2,\n",
       " 'test': 2,\n",
       " 'approval': 2,\n",
       " 'diagnose': 2,\n",
       " 'contribute': 2,\n",
       " 'follow': 2,\n",
       " 'agency': 2,\n",
       " 'determine': 2,\n",
       " 'private': 2,\n",
       " 'qualitative': 2,\n",
       " 'applicant': 2,\n",
       " 'advise': 2,\n",
       " 'datasets': 2,\n",
       " 'move': 2,\n",
       " 'achieve': 2,\n",
       " 'forecast': 2,\n",
       " 'acquisition': 2,\n",
       " 'commercialization': 2,\n",
       " 'administrative': 2,\n",
       " 'produce': 2,\n",
       " 'vendor': 2,\n",
       " 'bid': 2,\n",
       " 'familiarity': 2,\n",
       " 'attorney': 2,\n",
       " 'interpret': 2,\n",
       " 'utilize': 2,\n",
       " '20': 2,\n",
       " 'array': 2,\n",
       " 'headquarters': 2,\n",
       " 'core': 2,\n",
       " 'optimization': 2,\n",
       " 'meet': 2,\n",
       " 'disparate': 2,\n",
       " 'gap': 2,\n",
       " 'codm': 2,\n",
       " 'run': 2,\n",
       " 'year': 2,\n",
       " 'easy': 2,\n",
       " 'flexibility': 2,\n",
       " 'apply': 2,\n",
       " 'value': 2,\n",
       " 'ownership': 2,\n",
       " 'automate': 2,\n",
       " 'accuracy': 2,\n",
       " 'scale': 2,\n",
       " 'excel': 2,\n",
       " 'pace': 2,\n",
       " 'past': 2,\n",
       " 'proprietary': 2,\n",
       " 'senior': 2,\n",
       " 'coverage': 2,\n",
       " 'regional': 2,\n",
       " 'resolve': 2,\n",
       " 'ideal': 2,\n",
       " 'functionally': 2,\n",
       " 'los': 2,\n",
       " 'potential': 2,\n",
       " 'able': 2,\n",
       " 'capture': 2,\n",
       " 'cost': 2,\n",
       " 'main': 2,\n",
       " 'possible': 2,\n",
       " 'story': 2,\n",
       " 'evolution': 2,\n",
       " 'troubleshoot': 2,\n",
       " 'litigation': 2,\n",
       " 'task': 2,\n",
       " 'risk': 2,\n",
       " 'own': 2,\n",
       " 'execute': 2,\n",
       " 'compliance': 2,\n",
       " 'alternative': 2,\n",
       " 'time': 2,\n",
       " 'independently': 2,\n",
       " 'interested': 2,\n",
       " 'deep': 2,\n",
       " 'preparedness': 2,\n",
       " 'interview': 2,\n",
       " 'osha': 2,\n",
       " 'enhance': 2,\n",
       " 'iew': 2,\n",
       " 'recommend': 2,\n",
       " 'bi': 2,\n",
       " 'rule': 2,\n",
       " 'compare': 2,\n",
       " 'track': 2,\n",
       " 'district': 2,\n",
       " 'demonstrate': 2,\n",
       " 'network': 2,\n",
       " 'term': 2,\n",
       " 'communicate': 2,\n",
       " 'telephone': 2,\n",
       " 'collection': 2,\n",
       " 'appropriate': 2,\n",
       " 'large': 2,\n",
       " 'act': 2,\n",
       " 'fun': 2,\n",
       " 'strong': 2,\n",
       " 'angeles': 2,\n",
       " 'netflix': 2,\n",
       " 'documentation': 2,\n",
       " 'member': 2,\n",
       " 'take': 2,\n",
       " 'region': 2,\n",
       " '30': 2,\n",
       " 'measurement': 2,\n",
       " 'assurance': 2,\n",
       " 'acumen': 2,\n",
       " 'case': 2,\n",
       " 'attribute': 2,\n",
       " 'game': 2,\n",
       " 'trss': 2,\n",
       " 'application': 2,\n",
       " 'server': 2,\n",
       " 'mandate': 2,\n",
       " 'memo': 2,\n",
       " 'human': 2,\n",
       " 'lifecycle': 2,\n",
       " 'deliver': 2,\n",
       " 'make': 2,\n",
       " 'validate': 2,\n",
       " 'beyond': 2,\n",
       " 'intuitive': 2,\n",
       " 'drill': 2,\n",
       " 'towards': 2,\n",
       " 'visualization': 2,\n",
       " 'learn': 2,\n",
       " 'framework': 2,\n",
       " 'la': 2,\n",
       " 'governance': 2,\n",
       " 'accomplish': 2,\n",
       " 'space': 1,\n",
       " 'computer': 1,\n",
       " 'affect': 1,\n",
       " 'specific': 1,\n",
       " 'content': 1,\n",
       " 'mix': 1,\n",
       " 'rollout': 1,\n",
       " 'draw': 1,\n",
       " 'go': 1,\n",
       " 'inconsistency': 1,\n",
       " 'gamers': 1,\n",
       " 'shape': 1,\n",
       " 'field': 1,\n",
       " 'ready': 1,\n",
       " 'essential': 1,\n",
       " 'occurs': 1,\n",
       " 'proactive': 1,\n",
       " 'globally': 1,\n",
       " 'others': 1,\n",
       " 'extension': 1,\n",
       " 'introduction': 1,\n",
       " 'reception': 1,\n",
       " 'quickly': 1,\n",
       " 'segmentation': 1,\n",
       " 'evangelize': 1,\n",
       " 'comprehensive': 1,\n",
       " 'assessment': 1,\n",
       " 'junior': 1,\n",
       " 'anyone': 1,\n",
       " 'disperse': 1,\n",
       " 'inefficiency': 1,\n",
       " 'reveal': 1,\n",
       " 'search': 1,\n",
       " 'tell': 1,\n",
       " 'modify': 1,\n",
       " 'lock': 1,\n",
       " 'formulate': 1,\n",
       " 'turn': 1,\n",
       " 'ship': 1,\n",
       " 'disaster': 1,\n",
       " 'conjunction': 1,\n",
       " 'advice': 1,\n",
       " 'projection': 1,\n",
       " 'contribution': 1,\n",
       " 'transportation': 1,\n",
       " 'leadership': 1,\n",
       " 'verbally': 1,\n",
       " 'manner': 1,\n",
       " 'independent': 1,\n",
       " 'devise': 1,\n",
       " 'publicly': 1,\n",
       " 'annual': 1,\n",
       " 'available': 1,\n",
       " 'conversion': 1,\n",
       " 'effectively': 1,\n",
       " 'churn': 1,\n",
       " 'collaborate': 1,\n",
       " 'out': 1,\n",
       " 'interface': 1,\n",
       " 'incoming': 1,\n",
       " 'innovative': 1,\n",
       " 'firm': 1,\n",
       " 'friction': 1,\n",
       " 'open': 1,\n",
       " 'administrator': 1,\n",
       " 'fully': 1,\n",
       " 'curiosity': 1,\n",
       " 'particular': 1,\n",
       " 'brd': 1,\n",
       " '2': 1,\n",
       " 'place': 1,\n",
       " 'budgeting': 1,\n",
       " '120': 1,\n",
       " 'hypothesis': 1,\n",
       " 'proactively': 1,\n",
       " 'iteratively': 1,\n",
       " 'deadline': 1,\n",
       " 'resource': 1,\n",
       " 'outreach': 1,\n",
       " 'consensus': 1,\n",
       " 'pick': 1,\n",
       " 'confidential': 1,\n",
       " '5': 1,\n",
       " 'read': 1,\n",
       " 'capital': 1,\n",
       " 'late': 1,\n",
       " 'inventory': 1,\n",
       " 'assistance': 1,\n",
       " 'administration': 1,\n",
       " 'occasional': 1,\n",
       " 'differentiate': 1,\n",
       " 'immediately': 1,\n",
       " 'every': 1,\n",
       " 'amount': 1,\n",
       " 'monetization': 1,\n",
       " 'overnight': 1,\n",
       " 'geographically': 1,\n",
       " 'engineer': 1,\n",
       " 'adhere': 1,\n",
       " 'progressively': 1,\n",
       " 'award': 1,\n",
       " 'literature': 1,\n",
       " 'operates': 1,\n",
       " 'needle': 1,\n",
       " 'clean': 1,\n",
       " 'trader': 1,\n",
       " 'tremendous': 1,\n",
       " 'testimony': 1,\n",
       " 'accountable': 1,\n",
       " 'via': 1,\n",
       " 'health': 1,\n",
       " 'balance': 1,\n",
       " 'refresh': 1,\n",
       " 'standardization': 1,\n",
       " 'estimate': 1,\n",
       " 'instead': 1,\n",
       " 'departmental': 1,\n",
       " 'collaboration': 1,\n",
       " 'enables': 1,\n",
       " 'fellow': 1,\n",
       " 'load': 1,\n",
       " 'pipeline': 1,\n",
       " 'portion': 1,\n",
       " 'inspection': 1,\n",
       " 'month': 1,\n",
       " 'list': 1,\n",
       " 'ecosystem': 1,\n",
       " 'colleague': 1,\n",
       " 'confident': 1,\n",
       " 'answer': 1,\n",
       " 'investigation': 1,\n",
       " 'conclusion': 1,\n",
       " 'additionally': 1,\n",
       " 'cal': 1,\n",
       " 'music': 1,\n",
       " 'leader': 1,\n",
       " 'deployment': 1,\n",
       " 'internally': 1,\n",
       " 'vision': 1,\n",
       " 'passion': 1,\n",
       " 'dedicate': 1,\n",
       " 'country': 1,\n",
       " 'world': 1,\n",
       " 'hold': 1,\n",
       " 'agreement': 1,\n",
       " 'instrumental': 1,\n",
       " 'operating': 1,\n",
       " 'peer': 1,\n",
       " 'california': 1,\n",
       " 'tiktoks': 1,\n",
       " 'executes': 1,\n",
       " 'degree': 1,\n",
       " 'material': 1,\n",
       " 'grows': 1,\n",
       " 'externally': 1,\n",
       " 'observe': 1,\n",
       " 'weigh': 1,\n",
       " 'objective': 1,\n",
       " 'encumbrance': 1,\n",
       " 'creates': 1,\n",
       " 'highlight': 1,\n",
       " 'event': 1,\n",
       " 'efficiency': 1,\n",
       " 'startup': 1,\n",
       " 'real': 1,\n",
       " 'et': 1,\n",
       " 'duct': 1,\n",
       " 'supervise': 1,\n",
       " 'professional': 1,\n",
       " 'behavioral': 1,\n",
       " 'language': 1,\n",
       " 'copy': 1,\n",
       " 'store': 1,\n",
       " 'investment': 1,\n",
       " 'faster': 1,\n",
       " 'unlawful': 1,\n",
       " 'collaborative': 1,\n",
       " 'garbage': 1,\n",
       " 'highly': 1,\n",
       " 'accountability': 1,\n",
       " 'location': 1,\n",
       " 'several': 1,\n",
       " 'setup': 1,\n",
       " 'guide': 1,\n",
       " 'thorough': 1,\n",
       " 'uncover': 1,\n",
       " 'pickup': 1,\n",
       " 'commission': 1,\n",
       " 'window': 1,\n",
       " 'people': 1,\n",
       " 'oracle': 1,\n",
       " 'raw': 1,\n",
       " 'perspective': 1,\n",
       " 'outpost': 1,\n",
       " 'table': 1,\n",
       " 'champion': 1,\n",
       " 'file': 1,\n",
       " 'backlog': 1,\n",
       " 'distribute': 1,\n",
       " 'establish': 1,\n",
       " 'rigorous': 1,\n",
       " 'package': 1,\n",
       " 'find': 1,\n",
       " 'simply': 1,\n",
       " 'programmatic': 1,\n",
       " 'industry': 1,\n",
       " 'however': 1,\n",
       " 'roll': 1,\n",
       " 'hidden': 1,\n",
       " 'concurrent': 1,\n",
       " 'ground': 1,\n",
       " 'efficiently': 1,\n",
       " 'contributes': 1,\n",
       " 'digestible': 1,\n",
       " 'rapidly': 1,\n",
       " 'liaise': 1,\n",
       " 'cash': 1,\n",
       " 'meticulous': 1,\n",
       " 'recur': 1,\n",
       " 'sup': 1,\n",
       " 'stata': 1,\n",
       " 'chief': 1,\n",
       " 'context': 1,\n",
       " 'compelling': 1,\n",
       " 'attribution': 1,\n",
       " 'sheet': 1,\n",
       " 'variety': 1,\n",
       " 'martech': 1,\n",
       " 'often': 1,\n",
       " 'heavily': 1,\n",
       " 'keen': 1,\n",
       " 'facility': 1,\n",
       " 'approximately': 1,\n",
       " 'diverse': 1,\n",
       " 'creative': 1,\n",
       " 'continuous': 1,\n",
       " 'complete': 1,\n",
       " 'supervision': 1,\n",
       " 'background': 1,\n",
       " 'monitor': 1,\n",
       " 'coo': 1,\n",
       " 'econometric': 1,\n",
       " 'complaint': 1,\n",
       " 'distribution': 1,\n",
       " 'experiment': 1,\n",
       " 'point': 1,\n",
       " 'cultivate': 1,\n",
       " 'federal': 1,\n",
       " 'exceptional': 1,\n",
       " 'accordance': 1,\n",
       " 'powerpoint': 1,\n",
       " '10': 1,\n",
       " 'relationship': 1,\n",
       " 'outgo': 1,\n",
       " 'caliber': 1,\n",
       " '45': 1,\n",
       " 'crisis': 1,\n",
       " 'suggest': 1,\n",
       " 'complies': 1,\n",
       " 'manufacturing': 1,\n",
       " 'outcome': 1,\n",
       " 'm': 1,\n",
       " 'importance': 1,\n",
       " 'explorative': 1,\n",
       " 'scalable': 1,\n",
       " 'excellence': 1,\n",
       " 'monitoring': 1,\n",
       " 'paid': 1,\n",
       " 'range': 1,\n",
       " 'like': 1,\n",
       " 'gameswe': 1,\n",
       " 'room': 1,\n",
       " 'strategically': 1,\n",
       " 'depends': 1,\n",
       " 'exhibit': 1,\n",
       " 'underlie': 1,\n",
       " 'necessary': 1,\n",
       " '15': 1,\n",
       " 'graph': 1,\n",
       " 'non': 1,\n",
       " 'fricew': 1,\n",
       " 'associate': 1,\n",
       " 'net': 1,\n",
       " 'natural': 1,\n",
       " 'intersection': 1,\n",
       " 'cohesive': 1,\n",
       " 'allow': 1,\n",
       " 'always': 1,\n",
       " 'live': 1,\n",
       " 'guidance': 1,\n",
       " 'engage': 1,\n",
       " 'automobile': 1,\n",
       " 'geography': 1,\n",
       " 'cater': 1,\n",
       " '60': 1,\n",
       " 'flexible': 1,\n",
       " 'etc': 1,\n",
       " 'unit': 1,\n",
       " 'posse': 1,\n",
       " 'groundwork': 1,\n",
       " 'accommodation': 1,\n",
       " 'described': 1,\n",
       " 'pollen': 1,\n",
       " 'sa': 1,\n",
       " 'tactic': 1,\n",
       " 'extraction': 1,\n",
       " 'expertise': 1,\n",
       " 'someone': 1,\n",
       " 'ultimate': 1,\n",
       " 'decode': 1,\n",
       " 'gamer': 1,\n",
       " 'holistic': 1,\n",
       " 'relate': 1,\n",
       " 'budget': 1,\n",
       " 'without': 1,\n",
       " 'algorithm': 1,\n",
       " 'raise': 1,\n",
       " 'renovation': 1,\n",
       " 'income': 1,\n",
       " 'think': 1,\n",
       " 'adoption': 1,\n",
       " 'demand': 1,\n",
       " 'deeper': 1,\n",
       " 'nature': 1,\n",
       " 'fund': 1,\n",
       " 'life': 1,\n",
       " 'outside': 1,\n",
       " 'truth': 1,\n",
       " 'translate': 1,\n",
       " 'renewal': 1,\n",
       " 'one': 1,\n",
       " 'inter': 1,\n",
       " 'otc': 1,\n",
       " 'director': 1,\n",
       " 'soon': 1,\n",
       " 'reduce': 1,\n",
       " 'dar': 1,\n",
       " 'endevoursas': 1,\n",
       " 'oversees': 1,\n",
       " 'presentation': 1,\n",
       " 'overall': 1,\n",
       " 'cloud': 1,\n",
       " 'forward': 1,\n",
       " 'insightful': 1,\n",
       " 'user': 1,\n",
       " 'seek': 1,\n",
       " 'executive': 1,\n",
       " 'insurance': 1,\n",
       " 'repair': 1,\n",
       " 'dive': 1,\n",
       " 'situation': 1,\n",
       " 'consistent': 1,\n",
       " 'unparalleled': 1,\n",
       " 'creator': 1,\n",
       " 'speed': 1,\n",
       " 'long': 1,\n",
       " 'must': 1,\n",
       " 'incumbent': 1,\n",
       " 'exec': 1,\n",
       " 'impactful': 1,\n",
       " 'sense': 1,\n",
       " 'give': 1,\n",
       " 'distinct': 1,\n",
       " 'oneself': 1,\n",
       " 'con': 1,\n",
       " 'looker': 1,\n",
       " 'journey': 1,\n",
       " 'asset': 1,\n",
       " 'stretch': 1,\n",
       " 'reactively': 1,\n",
       " 'depth': 1,\n",
       " 'delivers': 1,\n",
       " 'codmobile': 1,\n",
       " 'face': 1,\n",
       " 'rda': 1,\n",
       " 'smarter': 1,\n",
       " 'professionally': 1,\n",
       " 'exposure': 1,\n",
       " 'property': 1,\n",
       " 'stock': 1,\n",
       " 'suite': 1,\n",
       " 'master': 1,\n",
       " 'recap': 1,\n",
       " 'interaction': 1,\n",
       " 'important': 1,\n",
       " 'channel': 1,\n",
       " 'eg': 1,\n",
       " 'urgency': 1,\n",
       " 'form': 1,\n",
       " 'stack': 1,\n",
       " 'another': 1,\n",
       " 'definition': 1,\n",
       " 'influence': 1,\n",
       " 'mentorship': 1,\n",
       " 'general': 1,\n",
       " 'ie': 1,\n",
       " 'end': 1,\n",
       " 'surface': 1,\n",
       " 'ideally': 1,\n",
       " 'item': 1,\n",
       " 'throughout': 1,\n",
       " 'assistant': 1,\n",
       " 'science': 1,\n",
       " 'regard': 1,\n",
       " 'short': 1,\n",
       " 'enhancement': 1,\n",
       " 'mysql': 1,\n",
       " 'promote': 1,\n",
       " 'solicit': 1,\n",
       " 'cleaning': 1,\n",
       " 'factor': 1,\n",
       " 'personal': 1,\n",
       " 'experimentation': 1,\n",
       " 'teach': 1,\n",
       " 'extensive': 1,\n",
       " 'obvious': 1,\n",
       " 'evidence': 1,\n",
       " 'matter': 1,\n",
       " 'safe': 1,\n",
       " 'liaison': 1,\n",
       " 'collect': 1,\n",
       " 'revel': 1,\n",
       " 'top': 1,\n",
       " 'witness': 1,\n",
       " 'ii': 1,\n",
       " 'trade': 1,\n",
       " 'daily': 1,\n",
       " 'type': 1,\n",
       " 'unfair': 1,\n",
       " 'rulemaking': 1,\n",
       " 'direction': 1,\n",
       " 'share': 1,\n",
       " 'rare': 1,\n",
       " 'agile': 1,\n",
       " 'collaboratively': 1,\n",
       " 'spread': 1,\n",
       " 'valuation': 1,\n",
       " 'stay': 1,\n",
       " 'inception': 1,\n",
       " 'approach': 1,\n",
       " 'significant': 1,\n",
       " 'proposal': 1,\n",
       " 'much': 1,\n",
       " 'administer': 1,\n",
       " 'retrieval': 1,\n",
       " 'word': 1,\n",
       " 'combine': 1,\n",
       " 'filing': 1,\n",
       " 'regularly': 1,\n",
       " 'usage': 1,\n",
       " 'career': 1,\n",
       " 'connect': 1,\n",
       " 'intrigue': 1,\n",
       " 'present': 1,\n",
       " 'status': 1,\n",
       " 'describe': 1,\n",
       " 'characteristic': 1,\n",
       " 'dynamic': 1,\n",
       " 'airplane': 1,\n",
       " 'renovate': 1,\n",
       " 'error': 1,\n",
       " 'emphasis': 1,\n",
       " 'concise': 1,\n",
       " 'add': 1,\n",
       " '25': 1,\n",
       " 'result': 1,\n",
       " 'deliverable': 1,\n",
       " 'roi': 1,\n",
       " 'credit': 1,\n",
       " 'receive': 1,\n",
       " 'officer': 1,\n",
       " 'primary': 1,\n",
       " 'commercial': 1,\n",
       " 'ramped': 1,\n",
       " 'upon': 1,\n",
       " 'hungry': 1,\n",
       " 'assortment': 1,\n",
       " 'multi': 1,\n",
       " 'basic': 1,\n",
       " 'communicates': 1,\n",
       " 'courier': 1,\n",
       " 'train': 1,\n",
       " 'youll': 1,\n",
       " 'consult': 1,\n",
       " 'install': 1,\n",
       " 'compile': 1,\n",
       " 'post': 1,\n",
       " 'remove': 1,\n",
       " 'become': 1,\n",
       " 'pilot': 1,\n",
       " 'embrace': 1,\n",
       " 'fresh': 1,\n",
       " 'lift': 1,\n",
       " 'oversee': 1,\n",
       " 'local': 1,\n",
       " 'entertainment': 1,\n",
       " 'feel': 1,\n",
       " 'postal': 1,\n",
       " 'contact': 1,\n",
       " 'social': 1,\n",
       " 'bar': 1,\n",
       " 'nest': 1,\n",
       " 'fix': 1,\n",
       " 'alternate': 1,\n",
       " 'group': 1,\n",
       " 'relation': 1,\n",
       " 'broadly': 1,\n",
       " 'market': 1,\n",
       " 'mitigation': 1,\n",
       " 'method': 1,\n",
       " 'arrange': 1,\n",
       " 'registration': 1,\n",
       " 'capability': 1,\n",
       " '12': 1,\n",
       " 'good': 1,\n",
       " 'instal': 1,\n",
       " '40lbs': 1}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wordcloud\n",
    "## Convert word frequencies to dictionary\n",
    "dict_for_wc = freq_lemma.to_dict()\n",
    "\n",
    "dict_for_wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the WordCloud image\n",
    "# makeImage(dict_for_wc, 'charlie_black.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give the wordcloud a specific shape, we need to use an image as a mask.  In order to do this, grab an image online and select only the part of the image you want to be the mask for the wordcloud. I used photoshop to select the shape I wanted:\n",
    "\n",
    "![Image after using selection tool in photoshop](charlie.png)\n",
    "\n",
    "Then fill the shape with black:\n",
    "\n",
    "![Image filled black](charlie_black.png)\n",
    "\n",
    "And now we can pass this image to our function `make_image()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the result:\n",
    "\n",
    "![jobbies](wc.png)\n",
    "\n",
    "Now we can [strap on our job helmets, squeeze down into a job cannon, and fire off into Jobland where jobs grow on jobbies](https://youtu.be/wbq571QME2Y?t=31)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
